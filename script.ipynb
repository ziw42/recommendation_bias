{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1b9ae0cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x233a82901b0>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pickle\n",
    "import torch\n",
    "import numpy as np\n",
    "import MF\n",
    "import math\n",
    "from sklearn import cluster\n",
    "from operator import itemgetter\n",
    "from math import log\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import time\n",
    "import argparse\n",
    "import pandas as pd\n",
    "import utility\n",
    "from scipy.sparse import csr_matrix, rand as sprand\n",
    "from tqdm import tqdm\n",
    "from MF import MF\n",
    "import VAE\n",
    "import os\n",
    "\n",
    "\n",
    "np.random.seed(0)\n",
    "torch.manual_seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9be74523",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Unpickling user and item matrices\n",
    "\n",
    "test_like = np.load(\"./data/user_vali_like.npy\", allow_pickle=True)\n",
    "train_like = np.load(\"./data/user_train_like.npy\", allow_pickle=True)\n",
    "\n",
    "### mtx_item[I]: the list of users that like item I\n",
    "### mtx_user[U]: the list of items that user U like\n",
    "\n",
    "with open(\"./data/item\", \"rb\") as f: \n",
    "    mtx_item = pickle.load(f)\n",
    "\n",
    "with open(\"./data/user\", \"rb\") as f:\n",
    "    mtx_user = pickle.load(f)\n",
    "    \n",
    "### Read the trained model\n",
    "model = torch.load(\"./data/save.pt\")\n",
    "# Item matrix\n",
    "Q = model.item_factors.weight.H.tolist()\n",
    "# User matrix\n",
    "P = model.user_factors.weight.H.tolist()\n",
    "\n",
    "P = np.array(P)\n",
    "Q = np.array(Q)\n",
    "\n",
    "### Predicted matrix\n",
    "Rec = np.matmul(P.T, Q)\n",
    "\n",
    "\n",
    "### The test_like and train_like lists, but for item\n",
    "### test_like_item[I]: the users who like item I in the test set\n",
    "### train_like_item[I]: the users who like item I in the train set\n",
    "test_like_item = []\n",
    "for t in range(Rec.shape[1]):\n",
    "    test_like_item.append([])\n",
    "i = 0\n",
    "for t in test_like:\n",
    "    for tt in t:\n",
    "        test_like_item[tt].append(i)\n",
    "    i += 1\n",
    "\n",
    "train_like_item = []\n",
    "for t in range(Rec.shape[0]):\n",
    "    train_like_item.append([])\n",
    "i = 0\n",
    "for t in train_like:\n",
    "    for tt in t:\n",
    "        train_like_item[tt].append(i)\n",
    "    i += 1\n",
    "    \n",
    "Rec_train = []\n",
    "for t in range(Rec.shape[0]):\n",
    "    row = [0]*Rec.shape[1]\n",
    "    for item in train_like[t]:\n",
    "        row[item] = 1\n",
    "    Rec_test.append(row)\n",
    "Rec_train = np.array(Rec_train)\n",
    "\n",
    "J_similar = Rec_train.dot(Rec_train.T)\n",
    "for t in range(J_similar.shape[0]):\n",
    "    J_similar[t][t] = 0\n",
    "for i in range(J_similar.shape[0]):\n",
    "    for j in range(i+1, J_similar.shape[1]):\n",
    "        if J_similar[i][j] != 0:\n",
    "            J_similar[i][j] /= (sum(Rec_train[i]) + sum(Rec_train[j]))\n",
    "for i in range(J_similar.shape[0]):\n",
    "    for j in range(i):\n",
    "        J_similar[i][j] = J_similar[j][i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2b377a18",
   "metadata": {},
   "outputs": [],
   "source": [
    "Rec_train_item = Rec_train.T\n",
    "J_similar_item = Rec_train_item.dot(Rec_train)\n",
    "for t in range(J_similar_item.shape[0]):\n",
    "    J_similar_item[t][t] = 0\n",
    "for i in range(J_similar_item.shape[0]):\n",
    "    for j in range(i+1, J_similar_item.shape[1]):\n",
    "        if J_similar_item[i][j] != 0:\n",
    "            J_similar_item[i][j] /= (sum(Rec_train_item[i]) + sum(Rec_train_item[j]))\n",
    "for i in range(J_similar_item.shape[0]):\n",
    "    for j in range(i):\n",
    "        J_similar[i][j] = J_similar[j][i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a2801048",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Functions for metrices\n",
    "def get_activeness(user):\n",
    "    return len(train_like[user])\n",
    "\n",
    "def get_popularity(item):\n",
    "    return len(train_like_item[item])\n",
    "\n",
    "def get_user_mainstreamness(user):\n",
    "    ### mainstreamness == jacarrd similarity\n",
    "    return(sum(J_similar[user])/(J_similar.shape[0]-1))\n",
    "\n",
    "def get_item_mainstreamness(item):\n",
    "    ### mainstreamness == jacarrd similarity\n",
    "    return(sum(J_similar_item[item])/(J_similar_item.shape[0]-1))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e3122e12",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Calculate the metrices\n",
    "\n",
    "activeness = []\n",
    "user_mainstreamness = []\n",
    "for t in range(Rec.shape[0]):\n",
    "    activeness.append(get_activeness(t))\n",
    "    user_mainstreamness.append((get_user_mainstreamness(t)))\n",
    "                               \n",
    "popularity = []\n",
    "item_mainstreamness = []\n",
    "for t in range(Rec.shape[1]):\n",
    "    popularity.append(get_popularity(t))\n",
    "    item_mainstreamness.append((get_item_mainstreamness(t)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9fa70bf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "### _sort[0] = values\n",
    "### _sort[1] = indexs\n",
    "\n",
    "act_sort = (np.sort(np.array(activeness)), np.argsort(np.array(activeness)))\n",
    "usrmain_sort = (np.sort(np.array(user_mainstreamness)), np.argsort(np.array(user_mainstreamness)))\n",
    "pop_sort = (np.sort(np.array(popularity)), np.argsort(np.array(popularity)))\n",
    "itmmain_sort = (np.sort(np.array(item_mainstreamness)), np.argsort(np.array(item_mainstreamness)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c826654e",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(\"./data/activeness.npy\", activeness)\n",
    "np.save(\"./data/user_mainstreamness.npy\", user_mainstreamness)\n",
    "np.save(\"./data/popularity.npy\", popularity)\n",
    "np.save(\"./data/item_mainstreamness.npy\", item_mainstreamness)\n",
    "\n",
    "np.save(\"./data/act_sort.npy\", act_sort)\n",
    "np.save(\"./data/usrmain_sort.npy\", usrmain_sort)\n",
    "np.save(\"./data/pop_sort.npy\", pop_sort)\n",
    "np.save(\"./data/itmmain_sort.npy\", itmmain_sort)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0804d5ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Load the metrics from disk\n",
    "act_sort = np.load(\"./data/act_sort.npy\")\n",
    "pop_sort = np.load(\"./data/pop_sort.npy\")\n",
    "usrmain_sort = np.load(\"./data/usrmain_sort.npy\")\n",
    "itmmain_sort = np.load(\"./data/itmmain_sort.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "79a52523",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Functions for calculating NDCG@K, performance, and average rank\n",
    "\n",
    "num_u = Rec.shape[0]\n",
    "like = train_like\n",
    "for i in range(num_u):\n",
    "    Rec[i, like[i]] = -100000.0\n",
    "\n",
    "### No train set in Rec_sort!!!\n",
    "### Pretty quick even we sort all users and items\n",
    "Rec_sort = []\n",
    "for t in Rec:\n",
    "    num = t.tolist().count(-100000.0)\n",
    "    tt = np.argsort(t)[::-1]\n",
    "    tt = tt[0:(len(tt)-num)]\n",
    "    Rec_sort.append(tt)\n",
    "\n",
    "def NDCG_at_k(predicted_list, ground_truth, k):\n",
    "    dcg_value = [(v / log(i + 1 + 1, 2)) for i, v in enumerate(predicted_list[:k])]\n",
    "    dcg = np.sum(dcg_value)\n",
    "    if len(ground_truth) < k:\n",
    "        ground_truth += [0 for i in range(k - len(ground_truth))]\n",
    "    idcg_value = [(v / log(i + 1 + 1, 2)) for i, v in enumerate(ground_truth[:k])]\n",
    "    idcg = np.sum(idcg_value)\n",
    "    return dcg / idcg\n",
    "\n",
    "def get_ndcg_ave(user):\n",
    "    k_set = [1,5,10,20]\n",
    "    u_pred = Rec[user, :]\n",
    "    top15_item_idx_no_train = np.argpartition(u_pred, -k_set[-1])[-k_set[-1]:]\n",
    "    top15 = (np.array([top15_item_idx_no_train, u_pred[top15_item_idx_no_train]])).T\n",
    "    top15 = sorted(top15, key=itemgetter(1), reverse=True)\n",
    "    new_user_prediction = top15\n",
    "    test = test_like[user]\n",
    "\n",
    "    dcg_list = []\n",
    "\n",
    "    # compute the number of true positive items at top k\n",
    "    rank_sum = 0\n",
    "    count = 0\n",
    "    for i in range(k_set[3]):\n",
    "        if new_user_prediction[i][0] in test:\n",
    "            rank_sum += (i+1)\n",
    "            count += 1\n",
    "            dcg_list.append(1)\n",
    "        else:\n",
    "            dcg_list.append(0)\n",
    "\n",
    "    # calculate NDCG@k\n",
    "    idcg_list = [1 for i in range(len(test))]\n",
    "    ndcg_tmp_15 = NDCG_at_k(dcg_list, idcg_list, k_set[3])\n",
    "    ### Calculate average rank of test items\n",
    "    ### Notice: only test items in the top 20 count\n",
    "    ave_rank = rank_sum/count\n",
    "    \n",
    "    return ndcg_tmp_15, ave_rank\n",
    "\n",
    "\n",
    "\n",
    "def get_performance_ave(item):\n",
    "### !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
    "### get performance@20 after 20 is 0\n",
    "    k = 20\n",
    "    test = test_like_item[item]\n",
    "    ### Number of the users who likes this item\n",
    "    hit_usr = len(test)\n",
    "    ### Top of the fraction\n",
    "    a = 0\n",
    "    \n",
    "    rank_sum = 0\n",
    "    count = 0\n",
    "    for t in range(k):\n",
    "        if t < hit_usr:\n",
    "            ranklist = Rec_sort[test[t]]\n",
    "            rank = ranklist.tolist().index(item)+1\n",
    "            rank_sum += rank\n",
    "            count += 1\n",
    "            a += 1/math.log(rank+2, 2)\n",
    "        else:\n",
    "            a += 0\n",
    "    performance = a/min(hit_usr, k) ### a/min(hit_usr, 20) or a/20?\n",
    "    ave_rank = rank_sum/count\n",
    "    \n",
    "    return performance, ave_rank\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f877d4e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_user():\n",
    "    user_num = Rec.shape[0]\n",
    "    NDCG = []\n",
    "    Ave_rank = []\n",
    "    for t in range(user_num):\n",
    "        ndcg_, ave_rank_ = get_ndcg_ave(t)\n",
    "        NDCG.append(ndcg_)\n",
    "        Ave_rank.append(ave_rank_)\n",
    "    return NDCG, Ave_rank\n",
    "\n",
    "def evaluate_item():\n",
    "    item_num = Rec.shape[1]\n",
    "    Performance = []\n",
    "    Ave_rank = []\n",
    "    for t in range(item_num):\n",
    "        performance_, ave_rank_ = get_performance_ave(t)\n",
    "        Performance.append(performance_)\n",
    "        Ave_rank.append(ave_rank_)\n",
    "    return Performance, Ave_rank\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0ae6479",
   "metadata": {},
   "outputs": [],
   "source": [
    "NDCG, Ave_rank_user = evaluate_user()\n",
    "Performance, Ave_rank_item = evaluate_item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "afae222e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_user(mtc, k):\n",
    "    ### mtc should be index of the users\n",
    "    \n",
    "    ### Divide users into k groups evenly\n",
    "    bt = int(len(mtc[0])/k)\n",
    "    labels = []\n",
    "    for t in range(k):\n",
    "        if t == k-1:\n",
    "            labels[t*bt::] = [t]*(len(mtc[0])-(k-1)*bt)\n",
    "        else:\n",
    "            labels[t*bt:t*(bt+1)] = [t]*bt\n",
    "        \n",
    "    NDCG = {}\n",
    "    Ave_rank = {}\n",
    "    ave_mtc = {}\n",
    "    m = {}\n",
    "    for t in range(k):\n",
    "        NDCG[t] = []\n",
    "        Ave_rank[t] = []\n",
    "        ave_mtc[t] = 0\n",
    "        n = 0\n",
    "        m[t] = []\n",
    "        for ii in range(len(labels)):\n",
    "            if labels[ii] == t:\n",
    "                ndcg_, ave_rank_ = get_ndcg_ave(mtc[1][ii])\n",
    "                NDCG[t].append(ndcg_)\n",
    "                Ave_rank[t].append(ave_rank_)\n",
    "                ave_mtc[t] += mtc[0][ii]\n",
    "                n += 1\n",
    "                m[t].append(mtc[0][ii])\n",
    "        ave_mtc[t] /= n\n",
    "    return NDCG, Ave_rank, ave_mtc, m\n",
    "\n",
    "def evaluate_item(mtc, k):\n",
    "    bt = int(len(mtc[0])/k)\n",
    "    labels = []\n",
    "    for t in range(k):\n",
    "        if t == k-1:\n",
    "            labels[t*bt::] = [t]*(len(mtc[0])-(k-1)*bt)\n",
    "        else:\n",
    "            labels[t*bt:t*(bt+1)] = [t]*bt\n",
    "    \n",
    "    performance = {}\n",
    "    ave_mtc = {}\n",
    "    m = {}\n",
    "    for t in range(k):\n",
    "        performance[t] = []\n",
    "        ave_mtc[t] = 0\n",
    "        n = 0\n",
    "        m[t] = []\n",
    "        for ii in range(len(labels)):\n",
    "            if labels[ii] == t:\n",
    "                performance[t].append(get_performance(mtc[1][ii]))\n",
    "                ave_mtc[t] += mtc[0][ii]\n",
    "                n += 1\n",
    "                m[t].append(mtc[0][ii])\n",
    "        ave_mtc[t] /= n\n",
    "    return performance, ave_mtc, m\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0869c06b",
   "metadata": {},
   "outputs": [],
   "source": [
    "### K is the number of groups\n",
    "k = 5\n",
    "bt_user = int(Rec.shape[0]/k)\n",
    "labels = []\n",
    "for t in range(k):\n",
    "    if t == k-1:\n",
    "        labels[t*bt_user::] = [t]*(len(usrmain_sort[0])-(k-1)*bt_user)\n",
    "    else:\n",
    "        labels[t*bt_user:t*(bt_user+1)] = [t]*bt_user\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7c507475",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "object of type 'numpy.float64' has no len()",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [19], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m groups \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m5\u001b[39m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m### here we do not need to calculate 4 times, call 1 time is enough\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m res_usrmain, ave_usrmain, m_usrmain \u001b[38;5;241m=\u001b[39m \u001b[43mevaluate_user\u001b[49m\u001b[43m(\u001b[49m\u001b[43musrmain_sort\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgroups\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      4\u001b[0m res_act, ave_act, m_act \u001b[38;5;241m=\u001b[39m evaluate_user(act_sort, groups)\n\u001b[0;32m      5\u001b[0m res_itmmain, ave_itmmain, m_itmmain \u001b[38;5;241m=\u001b[39m evaluate_item(itmmain_sort, groups)\n",
      "Cell \u001b[1;32mIn [18], line 7\u001b[0m, in \u001b[0;36mevaluate_user\u001b[1;34m(mtc, k)\u001b[0m\n\u001b[0;32m      4\u001b[0m k_ndcg \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m15\u001b[39m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m### Divide users into k groups evenly\u001b[39;00m\n\u001b[1;32m----> 7\u001b[0m bt \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mmtc\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m/\u001b[39mk)\n\u001b[0;32m      8\u001b[0m labels \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(k):\n",
      "\u001b[1;31mTypeError\u001b[0m: object of type 'numpy.float64' has no len()"
     ]
    }
   ],
   "source": [
    "groups = 5\n",
    "### here we do not need to calculate 4 times, call 1 time is enough\n",
    "res_usrmain, ave_usrmain, m_usrmain = evaluate_user(usrmain_sort, groups)\n",
    "res_act, ave_act, m_act = evaluate_user(act_sort, groups)\n",
    "res_itmmain, ave_itmmain, m_itmmain = evaluate_item(itmmain_sort, groups)\n",
    "res_pop, ave_pop, m_pop = evaluate_item(pop_sort, groups)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "3bb716c2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "### Convert results to dataframe\n",
    "def to_df(r, a, m):\n",
    "    df = {\"result\": [], \"metric\": [], \"group\": [], \"ave_metric\": []}\n",
    "    k = len(r)\n",
    "    for t in range(k):\n",
    "        l = len(r[t])\n",
    "        for tt in range(l):\n",
    "            df[\"result\"].append(r[t][tt])\n",
    "            df[\"metric\"].append(m[t][tt])\n",
    "            df[\"group\"].append(t)\n",
    "            df[\"ave_metric\"].append(a[t])\n",
    "    df = pd.DataFrame(df)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "3d5f7b86",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Save the dataframes\n",
    "df = to_df(res_pop, ave_pop, m_pop)\n",
    "df.to_csv(\"./data/item pop.csv\")\n",
    "df = to_df(res_usrmain, ave_usrmain, m_usrmain)\n",
    "df.to_csv(\"./data/user main.csv\")\n",
    "df = to_df(res_itmmain, ave_itmmain, m_itmmain)\n",
    "df.to_csv(\"./data/item main.csv\")\n",
    "df = to_df(res_act, ave_act, m_act)\n",
    "df.to_csv(\"./data/user act.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bf5d0379",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = to_df(res_act, ave_act, m_act)\n",
    "df.to_csv(\"./data/user act.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
